{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOdJJ2ecbSCg"
      },
      "source": [
        "# Creating a Face Swap App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbBqknZ_bWoo"
      },
      "source": [
        "In this project, we will utilize OpenCV and Dlib to detect and extract human faces from a given image. A pre-trained model will be employed to identify facial landmarks for precise face detection and processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFp7MeUuJRH5"
      },
      "source": [
        "# Importing necessary libraries\n",
        "import cv2  # For image processing and computer vision tasks\n",
        "import numpy as np  # For numerical operations and matrix manipulations\n",
        "import dlib  # For face detection and landmark extraction\n",
        "import requests  # For handling HTTP requests (e.g., downloading images)\n",
        "from PIL import Image  # For advanced image processing and handling\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def download_shape_predictor(destination=\"shape_predictor_68_face_landmarks.dat\"):\n",
        "    \"\"\"Downloads the Dlib shape predictor model if not already present.\"\"\"\n",
        "    url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "    compressed_file = destination + \".bz2\"\n",
        "\n",
        "    # Check if the file already exists\n",
        "    if os.path.exists(destination):\n",
        "        print(f\"{destination} already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading {destination}...\")\n",
        "    try:\n",
        "        # Download the compressed file\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(compressed_file, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "        print(\"Download complete. Extracting file...\")\n",
        "\n",
        "        # Extract the .bz2 file\n",
        "        import bz2\n",
        "        with bz2.BZ2File(compressed_file, \"rb\") as bz_file:\n",
        "            with open(destination, \"wb\") as out_file:\n",
        "                out_file.write(bz_file.read())\n",
        "\n",
        "        print(f\"{destination} successfully extracted.\")\n",
        "        # Optionally delete the compressed file\n",
        "        os.remove(compressed_file)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Download the shape predictor\n",
        "download_shape_predictor()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nUg58CEsZ28",
        "outputId": "61610be7-be70-4e35-92b0-83ab1102775c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading shape_predictor_68_face_landmarks.dat...\n",
            "Download complete. Extracting file...\n",
            "shape_predictor_68_face_landmarks.dat successfully extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOoqrAdYKGJ2"
      },
      "source": [
        "def extract_index_nparray(nparray):\n",
        "    \"\"\"\n",
        "    Extracts the first element from the first row of a NumPy array.\n",
        "\n",
        "    Parameters:\n",
        "        nparray (np.ndarray): A NumPy array with at least one row and one element.\n",
        "\n",
        "    Returns:\n",
        "        int or None: The first element of the array, or None if the array is empty.\n",
        "    \"\"\"\n",
        "    if nparray.size > 0:\n",
        "        return nparray[0, 0]\n",
        "    return None\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqIAab6JXCpo"
      },
      "source": [
        "Next, we will define a function to retrieve the index from a NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOumnR2oNTuS"
      },
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "def fetch_and_resize_image(url, size=(300, 300)):\n",
        "    \"\"\"\n",
        "    Fetches an image from the given URL, resizes it, and returns the processed image.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the image to fetch.\n",
        "        size (tuple): The target size for resizing the image (width, height).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: The resized image.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Fetch the image from the URL\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Open and resize the image\n",
        "        image = Image.open(response.raw)\n",
        "        image = image.resize(size)\n",
        "        return image\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the image: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the image: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example usage\n",
        "image_url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSx8Pu1tW1uCiZPfj9K1EL6uHxbg3bOKO9XkA&usqp=CAU'\n",
        "image1 = fetch_and_resize_image(image_url)\n",
        "if image1:\n",
        "    image1.show()  # Display the image (optional)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNA1qmsTXNzB"
      },
      "source": [
        "Next, we will fetch our source image from the internet using its URL and resize it to the desired dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMvalrv0O3he"
      },
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "def fetch_and_resize_image(url, size=(300, 300)):\n",
        "    \"\"\"\n",
        "    Fetches an image from the given URL, resizes it to the specified size, and returns the image.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the image to fetch.\n",
        "        size (tuple): The target size for resizing the image (width, height).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: The resized image or None if there was an error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send a request to get the image from the URL\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()  # Ensure we received a valid response\n",
        "\n",
        "        # Open and resize the image\n",
        "        image = Image.open(response.raw)\n",
        "        image = image.resize(size)\n",
        "        return image\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the image: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the image: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example usage\n",
        "image_url2 = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTYX1dyl9INRo5cbvDeTILRcZVzfcMsCsE0kg&usqp=CAU'\n",
        "image2 = fetch_and_resize_image(image_url2)\n",
        "if image2:\n",
        "    image2.show()  # Optionally display the image\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8w4LQ6IXb0g"
      },
      "source": [
        "Here, we will load the destination image from the internet using its URL and resize it to the desired dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6bMoYV_KI06"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def convert_to_gray(image):\n",
        "    \"\"\"\n",
        "    Converts a PIL Image to a NumPy array and then to grayscale.\n",
        "\n",
        "    Parameters:\n",
        "        image (PIL.Image.Image): The input image to be converted.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The grayscale image as a NumPy array.\n",
        "    \"\"\"\n",
        "    # Convert PIL Image to NumPy array\n",
        "    img_array = np.array(image)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    img_gray = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    return img_array, img_gray\n",
        "\n",
        "# Convert both images (image1 and image2) to arrays and grayscale\n",
        "img1_array, img1_gray = convert_to_gray(image1)\n",
        "img2_array, img2_gray = convert_to_gray(image2)\n",
        "\n",
        "# Create an empty mask for the first image\n",
        "mask = np.zeros_like(img1_gray)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i-29-leXjks"
      },
      "source": [
        "Now, we will convert our images into NumPy arrays and apply OpenCV to convert them into grayscale. Additionally, we will create an empty mask with the same shape as our source image, initialized with zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3qJ_JOoYB7r"
      },
      "source": [
        "Here we will first load Face detector and Face landmarks predictor using dlib and then we will find the height, width, channels which are required for creating empty image with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0alfXlyNKSsR"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_index_from_points(points, pt):\n",
        "    \"\"\"\n",
        "    Extracts the index of a point from the given array of points.\n",
        "\n",
        "    Parameters:\n",
        "        points (np.ndarray): The array of points.\n",
        "        pt (tuple): The point to find in the array.\n",
        "\n",
        "    Returns:\n",
        "        int or None: The index of the point, or None if the point is not found.\n",
        "    \"\"\"\n",
        "    # Use np.where to find the index of the point in the array\n",
        "    index = np.where((points == pt).all(axis=1))\n",
        "    return extract_index_nparray(index) if index[0].size > 0 else None\n",
        "\n",
        "# Face 1\n",
        "faces = detector(img_gray)\n",
        "for face in faces:\n",
        "    landmarks = predictor(img_gray, face)\n",
        "\n",
        "    # Extract landmark points\n",
        "    landmarks_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(0, 68)]\n",
        "\n",
        "    points = np.array(landmarks_points, np.int32)\n",
        "    convexhull = cv2.convexHull(points)\n",
        "    cv2.fillConvexPoly(mask, convexhull, 255)\n",
        "\n",
        "    # Masking the face region\n",
        "    face_image_1 = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "    # Delaunay triangulation\n",
        "    rect = cv2.boundingRect(convexhull)\n",
        "    subdiv = cv2.Subdiv2D(rect)\n",
        "    subdiv.insert(landmarks_points)\n",
        "    triangles = np.array(subdiv.getTriangleList(), dtype=np.int32)\n",
        "\n",
        "    indexes_triangles = []\n",
        "    for t in triangles:\n",
        "        pt1, pt2, pt3 = (tuple(t[i:i+2]) for i in range(0, len(t), 2))\n",
        "\n",
        "        # Extract indices of triangle points\n",
        "        index_pt1 = extract_index_from_points(points, pt1)\n",
        "        index_pt2 = extract_index_from_points(points, pt2)\n",
        "        index_pt3 = extract_index_from_points(points, pt3)\n",
        "\n",
        "        # If all triangle points are found, add the triangle to the list\n",
        "        if None not in [index_pt1, index_pt2, index_pt3]:\n",
        "            indexes_triangles.append([index_pt1, index_pt2, index_pt3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep6npRLbYvb8"
      },
      "source": [
        "First, we pass the image to the face detector, which is then used to extract landmarks using the shape predictor. The extracted landmark points (x and y coordinates) are stored in a list. Next, we segment the face into triangles. This step is essential for the face-swapping process, as each triangle will later be swapped with the corresponding triangle from the destination image. The triangulation of the destination image must match the pattern of the source image, meaning the connections between points must be identical. Therefore, after triangulating the source image, we extract the indices of the landmark points, which we use to replicate the same triangulation on the destination image. Once we have the triangle indices, we loop through them to triangulate the destination face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSnuuS0xKVRn"
      },
      "source": [
        "# Face 2\n",
        "faces2 = detector(img2_gray)\n",
        "for face in faces2:\n",
        "    # Extract landmarks for the second face\n",
        "    landmarks = predictor(img2_gray, face)\n",
        "    landmarks_points2 = []\n",
        "\n",
        "    # Loop through all 68 landmarks\n",
        "    for n in range(0, 68):\n",
        "        x = landmarks.part(n).x\n",
        "        y = landmarks.part(n).y\n",
        "        landmarks_points2.append((x, y))\n",
        "\n",
        "    # Convert landmarks to a NumPy array\n",
        "    points2 = np.array(landmarks_points2, dtype=np.int32)\n",
        "\n",
        "    # Create a convex hull for the second set of landmarks\n",
        "    convexhull2 = cv2.convexHull(points2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlttVbJ2bEmC"
      },
      "source": [
        "Next, we will apply a similar process as we did for the source image to extract landmarks from the destination image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw5j2VVLKX9D"
      },
      "source": [
        "# Creating empty masks\n",
        "source_face_mask = np.zeros_like(img_gray)  # Mask for the source image\n",
        "destination_face_mask = np.zeros_like(img2)  # Mask for the destination image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saE00buZbfsT"
      },
      "source": [
        "We will create empty images filled with zeros, which will be used for further processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGsw_KVnKY1d"
      },
      "source": [
        "# Triangulation of both faces\n",
        "for triangle_index in indexes_triangles:\n",
        "    # Extracting points for the first face\n",
        "    pt1_1 = landmarks_points[triangle_index[0]]\n",
        "    pt2_1 = landmarks_points[triangle_index[1]]\n",
        "    pt3_1 = landmarks_points[triangle_index[2]]\n",
        "    triangle1 = np.array([pt1_1, pt2_1, pt3_1], np.int32)\n",
        "\n",
        "    # Bounding rectangle and mask for the first face triangle\n",
        "    rect1 = cv2.boundingRect(triangle1)\n",
        "    (x1, y1, w1, h1) = rect1\n",
        "    cropped_triangle1 = img[y1: y1 + h1, x1: x1 + w1]\n",
        "    mask_triangle1 = np.zeros((h1, w1), np.uint8)\n",
        "\n",
        "    points1 = np.array([\n",
        "        [pt1_1[0] - x1, pt1_1[1] - y1],\n",
        "        [pt2_1[0] - x1, pt2_1[1] - y1],\n",
        "        [pt3_1[0] - x1, pt3_1[1] - y1]\n",
        "    ], np.int32)\n",
        "    cv2.fillConvexPoly(mask_triangle1, points1, 255)\n",
        "\n",
        "    # Visualizing triangles on the source image\n",
        "    cv2.line(source_face_mask, pt1_1, pt2_1, 255)\n",
        "    cv2.line(source_face_mask, pt2_1, pt3_1, 255)\n",
        "    cv2.line(source_face_mask, pt1_1, pt3_1, 255)\n",
        "\n",
        "    # Extracting points for the second face\n",
        "    pt1_2 = landmarks_points2[triangle_index[0]]\n",
        "    pt2_2 = landmarks_points2[triangle_index[1]]\n",
        "    pt3_2 = landmarks_points2[triangle_index[2]]\n",
        "    triangle2 = np.array([pt1_2, pt2_2, pt3_2], np.int32)\n",
        "\n",
        "    # Bounding rectangle and mask for the second face triangle\n",
        "    rect2 = cv2.boundingRect(triangle2)\n",
        "    (x2, y2, w2, h2) = rect2\n",
        "    mask_triangle2 = np.zeros((h2, w2), np.uint8)\n",
        "\n",
        "    points2 = np.array([\n",
        "        [pt1_2[0] - x2, pt1_2[1] - y2],\n",
        "        [pt2_2[0] - x2, pt2_2[1] - y2],\n",
        "        [pt3_2[0] - x2, pt3_2[1] - y2]\n",
        "    ], np.int32)\n",
        "    cv2.fillConvexPoly(mask_triangle2, points2, 255)\n",
        "\n",
        "    # Affine transformation from source triangle to destination triangle\n",
        "    points1 = np.float32(points1)\n",
        "    points2 = np.float32(points2)\n",
        "    transform_matrix = cv2.getAffineTransform(points1, points2)\n",
        "    warped_triangle = cv2.warpAffine(cropped_triangle1, transform_matrix, (w2, h2))\n",
        "    warped_triangle = cv2.bitwise_and(warped_triangle, warped_triangle, mask=mask_triangle2)\n",
        "\n",
        "    # Overlaying the warped triangle on the destination face\n",
        "    destination_face_area = img2_new_face[y2: y2 + h2, x2: x2 + w2]\n",
        "    destination_face_area_gray = cv2.cvtColor(destination_face_area, cv2.COLOR_BGR2GRAY)\n",
        "    _, mask_inverse = cv2.threshold(destination_face_area_gray, 1, 255, cv2.THRESH_BINARY_INV)\n",
        "    warped_triangle = cv2.bitwise_and(warped_triangle, warped_triangle, mask=mask_inverse)\n",
        "\n",
        "    destination_face_area = cv2.add(destination_face_area, warped_triangle)\n",
        "    img2_new_face[y2: y2 + h2, x2: x2 + w2] = destination_face_area\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-RSJkvMaoOk"
      },
      "source": [
        "Here, we perform similar operations for the destination image as we did for the source image. Once we have the triangulation for both faces, we extract the triangles from the source face. We also determine the coordinates of the corresponding triangles in the destination face. This allows us to warp the source face triangles to match the size and perspective of the corresponding triangles on the destination face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPHGGaFkKYAE"
      },
      "source": [
        "# Face swapping: Embedding the first face onto the second face\n",
        "face_mask = np.zeros_like(img2_gray)\n",
        "head_mask = cv2.fillConvexPoly(face_mask, convexhull2, 255)\n",
        "face_mask_inverse = cv2.bitwise_not(head_mask)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljukd3ECazF5"
      },
      "source": [
        "Once all the triangles have been cut and warped, the next step is to assemble them. We reconstruct the face using the same triangulation pattern, but this time, we replace each triangle with its warped counterpart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUJaa9ocKfos"
      },
      "source": [
        "# Removing the original face from the second image and replacing it with the swapped face\n",
        "head_without_face = cv2.bitwise_and(img2, img2, mask=face_mask_inverse)\n",
        "final_result = cv2.add(head_without_face, img2_new_face)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P15saueta6K7"
      },
      "source": [
        "The face is now prepared for replacement. We remove the face from the destination image to create space for the new face. Then, we combine the new face with the destination image without its original face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uyEucluKiRP"
      },
      "source": [
        "# Creating a seamless clone of the two faces\n",
        "(x, y, w, h) = cv2.boundingRect(convexhull2)\n",
        "center_of_face2 = (int((x + x + w) / 2), int((y + y + h) / 2))\n",
        "seamless_face_swap = cv2.seamlessClone(result, img2, face_mask, center_of_face2, cv2.NORMAL_CLONE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DjVUa18bB4K"
      },
      "source": [
        "Finally, the faces are successfully swapped, and it's time to adjust the colors to ensure the source face blends seamlessly with the destination image. OpenCV provides a built-in function called seamlessClone that automatically handles this operation. To apply it, we take the new face (created in the 6th step), use the original destination image and its mask to cut out the face, and determine the center of the face for proper alignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFPEQL8PKkaP"
      },
      "source": [
        "# Converting the array back to an image\n",
        "Image.fromarray(seamless_face_swap)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCYOaCEab6BQ"
      },
      "source": [
        "Finally, we will visualize the output by converting the numpy array into a Pillow Image object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQMB94U7eobp"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pFnxd8wetq_"
      },
      "source": [
        "We began by downloading a pretrained model for face landmarks and images from the internet to work on. Then, we utilized OpenCV and Dlib for image preprocessing, applying various functions to ultimately achieve the goal of swapping the face from the source image onto the destination image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjNtIH_rfNiD"
      },
      "source": [
        "# Scope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwjh5S6LfYGD"
      },
      "source": [
        "This project serves as an excellent resource for learning and understanding key concepts in computer vision. It can also be applied to build augmented reality applications, such as Snapchat, where face swapping and similar effects are commonly used."
      ]
    }
  ]
}